{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":65626,"databundleVersionId":8046133,"sourceType":"competition"}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-17T07:48:59.311667Z","iopub.execute_input":"2024-04-17T07:48:59.312463Z","iopub.status.idle":"2024-04-17T07:49:00.119415Z","shell.execute_reply.started":"2024-04-17T07:48:59.312431Z","shell.execute_reply":"2024-04-17T07:49:00.118671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import statements\n\nimport os\nimport sys\nimport pandas as pd\nimport numpy as np\nimport random\n\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport tensorflow as tf\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.applications.efficientnet import preprocess_input\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.layers import Dense, Input, Concatenate, Dropout\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\n\nfrom tensorflow.keras.mixed_precision import set_global_policy\nset_global_policy('mixed_float16')\n\nfor gpu in tf.config.experimental.list_physical_devices('GPU'):\n    tf.config.experimental.set_memory_growth(gpu, True)\n    \nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T04:27:23.266692Z","iopub.execute_input":"2024-04-29T04:27:23.267549Z","iopub.status.idle":"2024-04-29T04:27:23.275240Z","shell.execute_reply.started":"2024-04-29T04:27:23.267515Z","shell.execute_reply":"2024-04-29T04:27:23.274355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/planttraits2024/train.csv') # Puts the train.csv data into the variable train\n\nsd_columns = [col for col in train.columns if col.endswith('_sd')] # Drops the columns ending in _sd which are the trait columns\ntrain = train.drop(columns=sd_columns)\n\ntrain_image_folder = '/kaggle/input/planttraits2024/train_images' # sets a variable for the training images\ntrain['image_path'] = train['id'].apply(lambda x: os.path.join(train_image_folder, f\"{x}.jpeg\")) # Creates a variable that finds the image path given the id of the train data\n\ntest = pd.read_csv('/kaggle/input/planttraits2024/test.csv') # creates a variable for the teset data\ntest_image_folder = '/kaggle/input/planttraits2024/test_images' # sets a variable for the test images\ntest['image_path'] = test['id'].apply(lambda x: os.path.join(test_image_folder, f\"{x}.jpeg\")) # creates a variable that finds the image path given the id of the test data\n\nmean_columns = ['X4_mean', 'X11_mean', 'X18_mean', 'X50_mean', 'X26_mean', 'X3112_mean'] # creates column names for the graphs used later","metadata":{"execution":{"iopub.status.busy":"2024-04-29T04:27:26.857399Z","iopub.execute_input":"2024-04-29T04:27:26.858159Z","iopub.status.idle":"2024-04-29T04:27:28.746205Z","shell.execute_reply.started":"2024-04-29T04:27:26.858126Z","shell.execute_reply":"2024-04-29T04:27:28.745429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define image augmentation operations\ndef augment_image(img):\n    img = tf.image.random_flip_left_right(img)\n    img = tf.image.random_flip_up_down(img)\n    img = tf.image.random_brightness(img, max_delta=0.2)\n    img = tf.image.random_contrast(img, lower=0.5, upper=1.5)\n    img = tf.image.random_hue(img, max_delta=0.2)\n    img = tf.image.random_saturation(img, lower=0.5, upper=1.5)\n    img = tf.image.random_crop(img, size=[224, 224, 3])  # Random cropping\n    return img\n\n# Process image with augmentation\ndef process_image(file_path):\n    img = tf.io.read_file(file_path)\n    img = tf.image.decode_jpeg(img, channels=3) #Decodes a JPEG-encoded image to a tensor\n    img = tf.image.resize(img, [224, 224])\n    img = augment_image(img)  # Apply augmentation\n    img = preprocess_input(img) \n    return img\n\n# Define your dataset processing function\ndef process_path(file_path, tabular_data, targets):\n    img = process_image(file_path)\n    return (img, tabular_data), targets","metadata":{"execution":{"iopub.status.busy":"2024-04-29T04:27:31.758512Z","iopub.execute_input":"2024-04-29T04:27:31.758859Z","iopub.status.idle":"2024-04-29T04:27:31.767426Z","shell.execute_reply.started":"2024-04-29T04:27:31.758834Z","shell.execute_reply":"2024-04-29T04:27:31.766375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_data(df):\n    plt.figure(figsize=(15, 3))\n\n    # Setting up a grid of plots with 2 columns\n    n_cols = 6\n    n_rows = 1\n\n    for i, col in enumerate(mean_columns):\n        plt.subplot(n_rows, n_cols, i+1)\n        sns.kdeplot(df[col], bw_adjust=0.5, fill=False, color='blue')\n        plt.title(f'Distribution of {col}')\n        plt.xlabel('Value')\n        plt.ylabel('Density')\n\n    plt.tight_layout()\n    plt.show()\n    \nplot_data(train)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T04:27:33.495010Z","iopub.execute_input":"2024-04-29T04:27:33.495898Z","iopub.status.idle":"2024-04-29T04:27:36.444198Z","shell.execute_reply.started":"2024-04-29T04:27:33.495864Z","shell.execute_reply":"2024-04-29T04:27:36.443236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for column in mean_columns:\n    upper_quantile = train[column].quantile(0.98)  # Creates a variable for the top 2% of the data, the outliers\n    train = train[(train[column] < upper_quantile)] \n    train = train[(train[column] > 0)]    # creates a new training set where the values are above 0 and excluding the outliers\nplot_data(train) ","metadata":{"execution":{"iopub.status.busy":"2024-04-29T04:27:38.029675Z","iopub.execute_input":"2024-04-29T04:27:38.030672Z","iopub.status.idle":"2024-04-29T04:27:41.153394Z","shell.execute_reply.started":"2024-04-29T04:27:38.030635Z","shell.execute_reply":"2024-04-29T04:27:41.152470Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"original_means = {} # creates variables to hold the original means and standard deviations\noriginal_stds = {}\n\nfor column in mean_columns:\n    # Calculate the mean and standard deviation for each column\n    original_means[column] = train[column].mean()\n    original_stds[column] = train[column].std()\n    \n    # Apply the scaling: (value - mean) / std\n    # This standardizes each column to have a mean of 0 and std of 1\n    train[column] = (train[column] - original_means[column]) / original_stds[column]\n    \nplot_data(train)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T04:27:43.810082Z","iopub.execute_input":"2024-04-29T04:27:43.810763Z","iopub.status.idle":"2024-04-29T04:27:46.652925Z","shell.execute_reply.started":"2024-04-29T04:27:43.810731Z","shell.execute_reply":"2024-04-29T04:27:46.651906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = train.drop(columns=['id', 'image_path'] + mean_columns) # creates a variable for train that drops the id, image_path, and mean_columns\n\nfor column in x.columns: # Normalizes the data\n    min_val = x[column].min()\n    max_val = x[column].max()\n    x[column] = (x[column] - min_val) / (max_val - min_val)    ","metadata":{"execution":{"iopub.status.busy":"2024-04-29T04:28:07.283167Z","iopub.execute_input":"2024-04-29T04:28:07.283981Z","iopub.status.idle":"2024-04-29T04:28:07.454838Z","shell.execute_reply.started":"2024-04-29T04:28:07.283950Z","shell.execute_reply":"2024-04-29T04:28:07.454063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = train[mean_columns] # sets the output variable for the training set\nx_paths = train['image_path'] # creates a variable for tracking the image paths\n\ntrain_tabular, val_tabular, train_targets, val_targets = train_test_split( # Splits the training data into train and validation sets\n    x, y, test_size=0.2, random_state=42)\n\ntrain_paths, val_paths = train_test_split( # does the same split for the image paths\n    x_paths, test_size=0.2, random_state=42)\n\n# creates new dataset variables converting data to numpy arrays\ntrain_ds = tf.data.Dataset.from_tensor_slices((train_paths, train_tabular.to_numpy(), train_targets.to_numpy())) \nval_ds = tf.data.Dataset.from_tensor_slices((val_paths, val_tabular.to_numpy(), val_targets.to_numpy()))\n\n# Apply the processing function\ntrain_ds = train_ds.map(process_path, num_parallel_calls=tf.data.AUTOTUNE)\nval_ds = val_ds.map(process_path, num_parallel_calls=tf.data.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T05:15:24.186085Z","iopub.execute_input":"2024-04-29T05:15:24.186987Z","iopub.status.idle":"2024-04-29T05:15:24.617790Z","shell.execute_reply.started":"2024-04-29T05:15:24.186949Z","shell.execute_reply":"2024-04-29T05:15:24.616916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Image model pathway\nimage_input = Input(shape=(224, 224, 3)) \neffnet_layer = EfficientNetB0(include_top=False, weights='imagenet', input_tensor=image_input, pooling='avg') #  First layer is the pretrained model for the images\neffnet_layer.trainable = False  # Setting this false allows the model to continue using pre-trained features instead of re-training it \n\n# Tabular model pathway\ntabular_input = Input(shape=(train_tabular.shape[1],))\ntabular_dense = Dense(512, activation='relu')(tabular_input)\ntabular_dense = Dropout(0.5)(tabular_dense)  # Add dropout for regularization, reduces overfitting of the model\n\n# Concatenate both pathways\nconcat = Concatenate()([effnet_layer.output, tabular_dense])\nconcat_dense = Dense(256, activation='relu')(concat)\nconcat_dense = Dropout(0.5)(concat_dense)  # Continue to use dropout for regularization\n\n# Output layer for 6 targets\noutput = Dense(len(mean_columns), activation='linear')(concat_dense)  # Use linear activation for regression\n\nmodel = Model(inputs=[image_input, tabular_input], outputs=output)\n\nmodel.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n\n#model.summary()\nmodel","metadata":{"execution":{"iopub.status.busy":"2024-04-29T05:15:27.352293Z","iopub.execute_input":"2024-04-29T05:15:27.352651Z","iopub.status.idle":"2024-04-29T05:15:29.172754Z","shell.execute_reply.started":"2024-04-29T05:15:27.352625Z","shell.execute_reply":"2024-04-29T05:15:29.171774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(train_ds, validation_data=val_ds, epochs=50)","metadata":{"execution":{"iopub.status.busy":"2024-04-17T05:52:45.949873Z","iopub.execute_input":"2024-04-17T05:52:45.950253Z","iopub.status.idle":"2024-04-17T06:57:41.228187Z","shell.execute_reply.started":"2024-04-17T05:52:45.950218Z","shell.execute_reply":"2024-04-17T06:57:41.227377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save_weights(\"model.weights.h5\") # saves the weights from the model","metadata":{"execution":{"iopub.status.busy":"2024-04-17T06:57:41.230164Z","iopub.execute_input":"2024-04-17T06:57:41.230782Z","iopub.status.idle":"2024-04-17T06:57:41.680989Z","shell.execute_reply.started":"2024-04-17T06:57:41.230745Z","shell.execute_reply":"2024-04-17T06:57:41.680188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prepare test data (excluding 'id' and 'image_path')\ntest_tabular = test.drop(columns=['id', 'image_path'])\n\n#normalize test data\nfor column in test_tabular.columns:\n    min_val = test_tabular[column].min()\n    max_val = test_tabular[column].max()\n    test_tabular[column] = (test_tabular[column] - min_val) / (max_val - min_val)","metadata":{"execution":{"iopub.status.busy":"2024-04-17T06:57:41.682276Z","iopub.execute_input":"2024-04-17T06:57:41.682607Z","iopub.status.idle":"2024-04-17T06:57:41.789948Z","shell.execute_reply.started":"2024-04-17T06:57:41.682577Z","shell.execute_reply":"2024-04-17T06:57:41.788707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_tabular_np = test_tabular.to_numpy() # converts to numpy array\n\n# Create a TensorFlow dataset for the image paths and map them through the preprocessing function\ntest_images_ds = tf.data.Dataset.from_tensor_slices(test['image_path'])\\\n    .map(process_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n\n# Create a TensorFlow dataset for the tabular data\ntest_tabular_ds = tf.data.Dataset.from_tensor_slices(test_tabular_np)\n\n# Zip the two datasets together\ntest_ds = tf.data.Dataset.zip((test_images_ds, test_tabular_ds))\n\n# Prepare the dataset for prediction by ensuring the structure matches the model's expectations\ntest_ds_for_prediction = test_ds.map(lambda image, tabular: ((image, tabular),), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n\n# Use the model to predict on the dataset\npredictions = model.predict(test_ds_for_prediction)\n\npredictions_df = pd.DataFrame(predictions, columns=mean_columns)\ntest = pd.concat([test.reset_index(drop=True), predictions_df], axis=1) # creates variable to plot\n\nplot_data(test)\n\n#Verify we didn't predict NaNs..\nprint(\"NaN values\\n\", test[mean_columns].isna().sum())\ntest[mean_columns]","metadata":{"execution":{"iopub.status.busy":"2024-04-17T06:57:41.791206Z","iopub.execute_input":"2024-04-17T06:57:41.791563Z","iopub.status.idle":"2024-04-17T06:58:16.267073Z","shell.execute_reply.started":"2024-04-17T06:57:41.791521Z","shell.execute_reply":"2024-04-17T06:58:16.266196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for column in mean_columns:\n    original_mean = original_means[column]\n    original_std = original_stds[column]\n\n    # Reverse the standardization\n    test[column] = test[column] * original_std + original_mean\n\nplot_data(test)\ntest[mean_columns]","metadata":{"execution":{"iopub.status.busy":"2024-04-17T06:58:16.268324Z","iopub.execute_input":"2024-04-17T06:58:16.268650Z","iopub.status.idle":"2024-04-17T06:58:17.960203Z","shell.execute_reply.started":"2024-04-17T06:58:16.268623Z","shell.execute_reply":"2024-04-17T06:58:17.959361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = test[['id'] + mean_columns]\n\n#rename from _mean to fit the expectations of the competition\ntest.columns = test.columns.str.replace('_mean', '') \ntest.to_csv('submission.csv', index=False) # Puts the test df into submission.csv\n\ntest ","metadata":{"execution":{"iopub.status.busy":"2024-04-17T06:58:17.961216Z","iopub.execute_input":"2024-04-17T06:58:17.961509Z","iopub.status.idle":"2024-04-17T06:58:18.025374Z","shell.execute_reply.started":"2024-04-17T06:58:17.961474Z","shell.execute_reply":"2024-04-17T06:58:18.024519Z"},"trusted":true},"execution_count":null,"outputs":[]}]}